
# Kafka

## Kafka의 필요성
<img width="791" alt="complexity" src="https://user-images.githubusercontent.com/27878872/118393452-38320680-b67a-11eb-9bed-071608050df9.png">

* source system(producer) -> data(message) -> target system(consumer)
* 데이터 생산자(source system)와 데이터 소비자(target system)가 여러 개로 구성될 경우, 아래와 같이 복잡한 데이터 파이프라인 구성을 보인다.
* 서비스가 고도화되어 이렇게 복잡한 형태로 시스템이 구성될 경우, 여러 가지 문제점들이 있다.

#### 1. 데이터 통신(TCP, HTTP, REST, FTP, JDBC, etc)
* **프로토콜** 구성 및 관리가 어렵다
* 소스서버 A의 데이터를 여러 타겟서버들이 소비한다고 할 때, 각 타겟서버들이 서로 다른 프로토콜을 사용한다면, 소스서버 A는 이 모든 프로토콜들을 지원해야 한다.

#### 2. 데이터 포맷(Binary, JSON, CSV, etc)
* **데이터 포맷** 관리가 어렵다.
* 마찬가지로 각 서버들이 서로 다른 데이터 포맷을 사용하고 있는 경우, 이를 모두 지원해야 한다는 단점이 있다.

#### 3. 데이터 스키마와 확장성
* **데이터 스키마** 관리가 어렵고, 데이터가 미래에 다른 형태로 바뀔 때, 이에 대한 대응이 유연하지 못하다는 단점이 있다.

카프카가 이러한 이슈들을 해결해준다. source system과 target system을 디커플링 해준다. 카프카를 이용하면 각 서버는 자신의 역할에만 집중해서 개발하면 된다.
> 카프카 관련 키워드: producers, brokers(topics, partitions), zookeeper, consumers, messages

## Topics
* **A particular stream of data - 특정 데이터 스트림**
* database의 table과 유사
* 토픽 수에는 제한이 없어 원하는 만큼 만들 수 있음
* Identified by its **name(not number)**
* Split in partitions - 토픽은 파티션으로 쪼개짐

## Partitions
<img width="586" alt="topic_partition" src="https://user-images.githubusercontent.com/27878872/118393469-4ed85d80-b67a-11eb-82fc-bbe8fc642ade.png">

* Identified by its **number(not name)**
* 파티션 안에는 각 메시지의 위치를 나타내는 offset(오프셋)이라는 정보(incremental id)를 가지고 있다.
* Unbounded. e.g. 0,1,2, ...
* 같은 number(id)를 가지는 메시지는 없다. 메시지들은 서로 독립적이다.
* 데이터가 파티션으로 전달된다. key값을 전달받지 않으면 랜덤하게 파티션으로 할당된다.
* 데이터(message)가 한번 파티션에 쓰이면 바뀔 수 없다(**immutability**).
* 오프셋은 특정 파티션 내에서만 의미를 가진다.
* 파티션 내에서는 순서가 보장된다.
* 데이터는 특정 시간 동안만 보관되고 지나면 삭제된다
  * 예를 들어 1주일 동안 보관되었다가 그 이후엔 삭제된다.

## Topic example: trucks_gps
<img width="715" alt="kafka_truck_example" src="https://user-images.githubusercontent.com/27878872/118393494-77605780-b67a-11eb-8c82-b4b3298ce062.png">

* 여러 대의 트럭들이 있다고 할 때, 각 트럭은 자신의 GPS 위치 정보를 카프카에 보내게 됨.
* 이 때, 모든 트럭들의 위치 정보를 가지고 있는 trucks_gps 라는 토픽이 있을 수 있음.
* 각 트럭은 20초마다 카프카에 메시지를 보내게 되고 각 메시지는 트럭의 id와 위치 정보를 포함하고 있음.
* 토픽은 임의의 파티션들로 쪼개질 수 있음.
* truck - kafka - location dashboard
* truck - kafka - notification service

## Brokers
<img width="741" alt="broker_example" src="https://user-images.githubusercontent.com/27878872/118393507-81825600-b67a-11eb-883f-7f25d434cb7e.png">

* 브로커는 토픽(그리고 파티션)을 hold하고 있는 주체
* 카프카는 토픽을 만들면 자동적으로 모든 브로커들에게 토픽을 파티션 단위로 나눠서 분산해서 할당하여 저장한다.
* 카프카 클러스터는 여러 개의 브로커(서버)들로 구성된다. 그리고 브로커는 기본적으로 서버.
* Identified by its **id(number, integers, not string)**
* 각 브로커는 토픽에 메시지(데이터)를 저장하는데, 파티션들을 나누어서 저장한다.
* 카프카는 distributed이므로 전체 데이터를 가지고 있을 필요가 없고, 한 토픽의 파티션이 3개일 때, 3개의 브로커가 각 파티션을 나누어서 저장한다.
* 클러스터 내의 브로커 개수는 보통 3개 이상이며 브로커의 개수가 100개가 넘는 클러스터도 있다.

## Topic Replication Factor
<img width="743" alt="partition_replication" src="https://user-images.githubusercontent.com/27878872/118393516-8fd07200-b67a-11eb-887b-760ff8a7a9c7.png">

* 카프카는 분산 시스템이다. (= 브로커(서버) 여러 대가 하나의 카프카 시스템을 구성)
* 따라서 서버 죽었을 때에도 작업이 돌아가야 하기 때문에, 이에 대비하여 파티션의 카피본을 다른 서버에 저장한다. 이를 레플리케이션이 한다. 레플리케이션은 데이터 유실이 발생하지 않음을 보장해줄 수 있다.
* 파티션 레플리케이션끼리는 서로 linked 되어 있다.
* 여러 브로커가 한 파티션을 가지고 있을 때, 주어진 파티션에 대해서는 한 브로커만 리더가 될 수 있다.
* **Replication Factor** : 몇 개의 카피를 저장할지를 결정하는 값
  * 보통 replication factor로 3이 권장 - 동알힌 파티션에 대해 3개의 카피를 여러 브로커들이 분산해서 저장.
* Leader와 ISR(In-Sync Replica)
  * **Leader** : 동일한 파티션을 여러 브로커가 가질 수 있기에, **데이터 수신 및 송신을 담당하는** 리더가 있어야 한다. 리더 파티션만이 해당 파티션 관련 데이터를 받고 보낼 수 있다. 그 외의 카피본(ISR)들은 리더 파티션의 데이터를 싱크한다. 만약 리더 파티션이 있는 브로커가 죽으면 다른 브로커에 있는 ISR이 리더가 되고, 원래 리더 파티션이 있는 브로커가 살아나면 해당 파티션이 다시 리더가 되는데, 이는 Zookeeper에 의해 결정된다.

## Producers
<img width="766" alt="producer" src="https://user-images.githubusercontent.com/27878872/118393520-93fc8f80-b67a-11eb-98d1-221ddb9e6b62.png">

* 데이터를 생산하여, 카프카로 데이터를 전송하는 주체
  * 프로듀서가 파티션으로 구성된 토픽에 데이터를 **기록(write)**.
  * 프로듀서는 해당 토픽과 관련하여 어느 브로커의 어떤 파티션에 데이터를 기록(write)할지 **자동으로** 알고 있다.
* 프로듀서의 로드밸런싱
  * 키를 명시하지 않으면, 라운드 로빈 방식으로 데이터를 각 브로커에 분산한다.

### 전송 보장 방식 (QoS)
* 프로듀서가 데이터가 제대로 write되었는지를 acks에 따라 브로커로부터 확인을 요구
* acks=0
  * 프로듀서는 ack를 기다리지 않는다. => 데이터 유실 가능성 존재
* acks=1
  * 프로듀서는 리더의 ack만 기다린다.(디폴트 기능) => 리더가 다운되었을 경우에는 데이터 유실 가능성 존재
* acks=all
  * 프로듀서₩는 리더와 나머지 레플리카들의 ack를 기다린다. => 데이터 유실 없음

### Message Keys
* 프로듀서는 브로커에 메시지(데이터)를 보낼 때, **키**를 같이 보낼 것인지를 선택할 수 있다.
* 키가 없으면, 라운드 로빈 방식으로 각 브로커에 write된다.
* 키가 있으면, 해당 키값을 가지고 있는 모든 메시지는 동일한 파티션으로 보내진다.
* 특정 키가 "동일한" 파티션으로 가도록은 보장된다. (KEY HASHING)

## Consumers
<img width="689" alt="consumer" src="https://user-images.githubusercontent.com/27878872/118393538-a4ad0580-b67a-11eb-8ed6-31fefc4638a4.png">

* 토픽에서 데이터를 읽어서 소비하는 주체
* 컨슈머는 해당 토픽과 관련하여 어느 브로커에서 데이터를 읽을지 알고 있다.
* 컨슈머는 브로커가 다운되었을 때 이를 recover 가능
* **데이터는 파티션 내에서 순서대로 소비된다.**
* 하나의 컨슈머는 두 개 이상의 브로커에서 데이터를 읽어올 수 있다.
  * 이 때, 각 브로커의 파티션을 parrallel하게 읽어오며, 파티션 간의 데이터 읽기의 순서는 보장할 수 없다.

### Consumer Groups
<img width="808" alt="consumer_group" src="https://user-images.githubusercontent.com/27878872/118393542-abd41380-b67a-11eb-8efd-d11964fe5785.png">

* 컨슈터는 컨슈머 그룹 단위로 묶인다.
* 그룹 내의 컨슈머들은 서로 안겹치게 배타적으로 파티션을 점유해서 데이터를 소비
  * 하나의 파티션은 하나의 컨슈머에만 연결된다.
  * 그렇기 때문에 파티션들보다 그룹 내의 컨슈머가 더 많다면 몇 몇 컨슈머들은 비활성화 상태에 있게 된다.
  * 따라서 파티션 수와 컨슈머 그룹 내의 컨슈머 개수를 맞추는 것을 권장

### Consumer Offsets
<img width="709" alt="consumer_offset" src="https://user-images.githubusercontent.com/27878872/118393545-b1c9f480-b67a-11eb-93da-a95169996265.png">

* 컨슈머를 위한 기능으로 체크포인팅, 북마킹하는 개념 - **commit**
* 카프카는 자체적으로 컨슈머가 어느 오프셋까지 읽었는지 기억(저장)한다.
* 해당 오프셋은  __consumer_offsets이라는 카프카 토픽에 라이브로 커밋되어서 저장된다.
* 컨슈머는 카프카에서 받은 데이터를 **소비 완료**했으면 카프카로 커밋을 보내야 한다.
* 컨슈머가 다운되더라도 __consumer__offsets 덕분에 다운된 시점의 오프셋으로 다시 되돌아가서 데이터를 읽을 수 있다. => 다운된 시점의 오프셋이 __consumer_offsets에 커밋되었기 때문에

#### Delivery Semantics for Consumers
* 컨슈머는 어느 시점에 오프셋 커밋을 할 것인지 선택할 수 있다.
* 3가지 딜리버리 시멘틱
  * **At most once**
    * 오프셋 커밋 명령을 컨슈머가 **데이터를 수신한 시점**에 보낸다.
    * 컨슈머가 데이터를 처리하는 과정에서 문제가 발생했을 때, 데이터를 받자마자 커밋을 했기 때문에 그 데이터를 잃게 된다.
  * **At least once(usually preferred)**
    * 오프셋 커밋 명령을 컨슈머가 **데이터를 처리 완료한 시점**에 보낸다.
    * 데이터 처리 과정에서 문제가 발생했을 때, 그 데이터를 다시 받아올 수 있다.
    * 이렇게 되면 데이터들을 중복해서 처리할 가능성이 있다.
      * 그렇기 때문에 데이터 처리과정은 **idempotent**가 보장되어야 한다.
      * 즉, 데이터들을 다시 처리했을 때 전체 시스템에 영향을 끼치지 않아야 한다.
  * **exactly once**
    * Can be achieved for kafka => 실제 서비스 차원에서는 잘 안쓰이고 카프카 끼리, 혹은 카프카 내에서 내부적으로 쓰이는 딜리버리 시멘틱

## Kafka Broker Discovery
<img width="728" alt="kafka_broker_discovery" src="https://user-images.githubusercontent.com/27878872/118393552-babac600-b67a-11eb-8e1b-4e4406280634.png">

* 카프카는 자동으로 어떤 브로커의 파티션에 데이터를 저장할지, 어떤 브로커의 파티션에서 데이터를 읽을지 안다. 그것이 가능한 이유는?
* 각 브로커는 **bootstrap server** 라고 불리는다. 즉, 하나의 브로커에 연결하기만 하면 전체 클러스터에 연결된다.
  * 이는 모든 브로커가 다른 모든 브로커의 정보(토픽, 파티션과 관련된 **메타데이터**)를 가지고 있기 때문이다.
  * 어떤 브로커가 어떤 토픽의 어떤 파티션을 가지는지에 관한 **메타데이터**
* 따라서 클라이언트(프로듀서, 컨슈머)는 오로지 하나의 브로커에만 연결되면 된다.
* 클라이언트 API는 연결된 브로커에게 메타데이터 정보를 질의하고 이를 바탕으로 클라이언트는 연결해야 하는 브로커에게 다시 접근하게 된다. 단계별로 설명하자면 다음과 같다.
* 1. 카프카 클라이언트가 카프카 클러스터에게 **connection과 metadata**를 요청한다
* 2. **kafka cluster가 metadata(list of all brokers, ip, port, etc)를 클라이언트에게 준다.**
* 3. 그러고 나면 클라이언트가 데이터를 생산하거나 소비할 때, 어떤 브로커와 연결해야하는지 자동적으로 알게 되고 이를 바탕으로 연결해서 처리를 한다.

## Zookeeper
<img width="773" alt="zookeeper" src="https://user-images.githubusercontent.com/27878872/118393560-c4442e00-b67a-11eb-89b1-a2d72ddaab18.png">

* 주키퍼는 브로커들을 관리한다. (브로커 리스트 등) => 카프카 시스템의 핵심
  * 주키퍼는 파티션들에 대한 리더
  * 주키퍼는 각 종 *이벤트(e.g. new topic, broker dies, broker comes up, delete topics, etc, ...)* 를 카프카 시스템으로 **notification**을 보낸다.
* 실제 서비스 환경에서 주키퍼는 홀수개(3, 5, 7)의 서버로 운용된다.
* 주키퍼에는 **데이터 기록(write)을 핸들링하는 leader** 와 **데이터 읽기(read)를 핸들링하는 나머지 서버들인 followers가 있다.**
* 그러나 주키퍼는 fail-over 용도 등에 활용되고,
* 클라이언트로의 메타데이터 서빙은 브로커가 직접 담당한다.
* 주키퍼는 더 이상 컨슈머 오프셋들을 저장하지 않는다.(kafka > v0.10)

## Summary
<img width="786" alt="summary" src="https://user-images.githubusercontent.com/27878872/118393563-cc9c6900-b67a-11eb-93eb-cd8801861b12.png">

## Reference
> https://www.udemy.com/course/apache-kafka/
