## 카프카의 필요성



### 1. 구성을 간단하게 하여, 관리가 용이해진다. 



#### As-Is

* 데이터 생산자와 소비자가 여러개로 구성될 경우 아래와 같이 복잡한 데이터 파이프라인 구성을 보인다.

![image](https://user-images.githubusercontent.com/34915108/117964874-204a4200-b35d-11eb-85a8-9b8744c68ae4.png)

* **프로토콜** 구성 및 관리가 어렵다. 
  * 소스서버A의 데이터를 타겟서버A, 타겟서버B가 소비하고자 할 때, 
  * 각각 다른 프로토콜을 사용한다면, 
  * 소스서버A는 두 가지 프로토콜을 지원해야 할 것이다. (BAD)
* **데이터 포멧** 관리가 어렵다. 
  * 위와 비슷한 논리로, 생산자 측에서 여러 포멧을 지원해야 하는 경우 이를 모두 지원하는 것은 나쁘다.(BAD)
* **소스 서버의 부하**가 증가하게 된다.
  * 소스서버에 붙게되는 타겟서버가 많아질수록 커넥션은 증가하게 될 것이다. (BAD)



#### To-Be

* **디커플링** 
  * 카프카를 통하 **데이터 스트림**과 **시스템**을 분리할 수 있게 된다. 



![image](https://user-images.githubusercontent.com/34915108/117964917-2c360400-b35d-11eb-99a2-2bfecc95e2f3.png)

* 극복
  * **프로토콜** : 소스서버에서 하나의 프로토콜만 지원하면 된다. 
  * **데이터 포멧** : 소스서버에서 하나의 데이터 포멧으로 생상하면 된다.
  * **소스서버의 부하** : 카프카와의 커넥션만 유지하면 된다. 
* 그외 장점(이자 특징)
  * Distributed :
  * resilent architecture :
  * fault tolerant :
  * horizontal scalability
    * up to 100s of brokrer
    * millions of message per second (거의 실시간 - real time)





# Kafka Theory



## 1. Topics, Partitions and Offsets



![image](https://user-images.githubusercontent.com/34915108/117964968-37892f80-b35d-11eb-8fae-b3bffc39be15.png)



* **Topic**
  * Stream of Data : 토픽을 하나의 데이터 스트리밍 단위로 보자. 
  * Tbl in DB 와 비슷한 개념
  * **name** 으로 구분
* **Partition**
  * 토픽은 파티션으로 나눠진다. 
* **Offset** 
  * 파티션 내의 각 메세지는 증가하는 id로 채번
  * 무한히 증가가능 (HOW)
  * (디폴트) 오프셋에 해당하는 데이터는 일주일 후 제거됨.



* 데이터가 파티션 내에 쓰여지면, 그 순간 바꿀 수 없다. (Immutability)

* (키값을 부여하지 않는다면) 토픽 내의 여러 파티션 중 어느 파티션에 데이터가 저장될지는 **랜덤** 





## Broker



![image](https://user-images.githubusercontent.com/34915108/117965080-54bdfe00-b35d-11eb-9571-3d2f47392021.png)



* **카프카 클러스터** : 여러개의 브로커, 여러 대의 서버로 구성된다.
* **브로커** 
  * 각 브로커는 고유한 정수 ID로 구분됨.
  * 각 브로커는 한 토픽의 파티션들을 나누어서 저장함.
    * (아래와 같이) 한 토픽의 파티션이 3개일 때, 3개의 브로커는 각 파티션을 나누어서 저장함.

![image](https://user-images.githubusercontent.com/34915108/117965124-5daecf80-b35d-11eb-9711-0eee82e1c61f.png)


* **의문** 
  * Broker 102가 다운되면 토픽A의 파티션2의 데이터를 순간적으로 접근 불가한가??
    * 1) 파워 나가서, 
    * 2) JVM GC 발생으로, 
    * 3) VM장비에서 다른 서버의 IO 증가 영향, 
    * 4)  디도스 공격 => 요청량 몰림 
    * 5) 준비한 장비대수로, 급작스럽게 몰린 트래픽을 감당하지 못 해서 => OOM 발생





## Topic Replication

* 카프카는 분산 시스템이다. (= 브로커 여러대가 하나의 카프카 시스템을 구성)



![image](https://user-images.githubusercontent.com/34915108/117965165-6acbbe80-b35d-11eb-8198-ac97801d6055.png)



* 서버 죽었을 때를 대비하여, 파티션의 카피본을 다른 서버에 저장한다. 

* **replication factor** : 몇 카피를 저장할 결정하는 값 





* **Leader** : 동일한 파티션을 여러 브로커가 가질 수 있기에, "데이터 수신 및 송신을 담당하는" 리더가 있어야 한다. 
  * ex) ""토픽A의 파티션2의 리더는 broker102이다..."
  * 그외 카피본은 리더의 파티션의 데이터를 싱크한다. (HOW? )



![image](https://user-images.githubusercontent.com/34915108/117965198-74552680-b35d-11eb-8c50-78a19843be3a.png)






## Producer

* 데이터를 생산하여, 카프카로 전송하는 주체
  * producer가 (파티션으로 구성된) 토픽에 데이터를 쓴다.
  * producer는 토픽을 가지는 "어느" 브로커에 쓸지 "자동으로" 알고 있다.
  * 브로커 failure 발생 시 producer will automatically recover (HOW??)

* producer will load balance (??)
  * 키를 명시하지 않으면, RR 방식으로 데이터를 각 브로커에 분산한다. 

![image](https://user-images.githubusercontent.com/34915108/117965296-8df66e00-b35d-11eb-9c8a-fb286f8331a8.png)



### 전송 보장 방식 (QoS)

* producer가 선택할 수 있음
  * 무엇을? 브로커로부터 "데이터가 제대로 write되었는지"
* acks=0
  * producer는 ack를 기다리지 않는다. => 데이터 유실 가능
* acks=1
  * producer는 리더의 ack만 기다린다. (디폴트 기능)
* acks=all
  * producer는 리더 + 레플리카 의 ack를 기다린다. => 데이터 유실 없음(확실?)





### Message Keys 

* producer는 **키**를 선택할 수 있다.
* 키가 없으면, RR방식으로 각 브로커에 write된다. 

![image](https://user-images.githubusercontent.com/34915108/117965335-9c448a00-b35d-11eb-849f-091468606591.png)


* 특정 키가 "특정" 파티션으로 가도록은 제어할 수 없다.
* 특정 키가 "동일한" 파티션으로 가도록은 보장된다. (KEY HASHING)





## Consumer

* consumer는 토픽에서부터 데이터를 읽는다. 
* consumer는 어느 브로커에서 데이터를 읽을지 알고 있다. 
* 그 브로커가 다운 시, recover 가능 (that means... )
* **데이터는 파티션 내에서 순서대로 소비된다.** 
  * 파티션내의 offset 3을 offset 1보다 먼저 소비할 수 없다. 

10![image](https://user-images.githubusercontent.com/34915108/117965396-ad8d9680-b35d-11eb-80a2-898c2736a634.png)


* 하나의 consumer는 두 개 이상의 브로커에서 데이터를 읽어올 수 있다. 
  * 이때, 각 브로커의 파티션은 read in parallel
  * 파티션 간의 데이터 읽기 순서는 보장할 수 없다(???)



### Consumer Groups 

* consumer는 consumer group 단위로 묶는다. 
* 그룹 내의 consumer들은 배타적으로 파티션을 점유해서, 데이터를 소비한다. 
  * 하나의 파티션당 오로지 하나의 conumser에만 연결된다.



11![image](https://user-images.githubusercontent.com/34915108/117965434-b716fe80-b35d-11eb-8bee-aac78eac0426.png)


* 그래서 파티션 수와 consumer group내의 consumer 개수를 맞추는 것이 권장. 





## Consumer Offset

#체크포인팅, 북마 #commit하는 개념 #consumer를 위한 기능 



* 카프카는 자체적으로 consumer가 어느 offset까지 읽었는지를 저장(기억)한다. 
* 카프카에 저장되는 offset 형식은 __consumer_offsets 
* consumer는 카프카에서 받은 데이터를 소비 "완료"했으면, 카프카로 커밋을 날려야 한다. 



12![image](https://user-images.githubusercontent.com/34915108/117965451-baaa8580-b35d-11eb-9423-76df85b82850.png)


* 이를 통해 consumer가 다운되더라도, 카프카에서 어디까지 읽었는지 기억하기에 그 값을 활용해서 다시 offset을 소비하면 된다. 





### Delivery semantics for consumers

* offset commit을 어느 시점에 날릴 것인가...



* **At most once** 
  * offset commit 명령은 consumer가 **데이터를 수신한 시점**에 보낸다. 
  * 데이터를 처리하는 시점에 문제가 발생했을 때, 다시 데이터를 받아올 수는 없다. 
* **At least once** 
  * offset commit 명령은 consumer가 **데이터를 처리 완료한 시점**에 보낸다. 
  * 데이터 처리 과정에서 문제 발생 시, 다시 그 데이터를 받아올 수 있다. 
  * (중요) 이 단계는 consumer측에서 데이터를 중복해서 수신이 가능하다. 
    * 그렇기에 데이터 처리과정은 idempotent가 보장되어야 한다. 
* **exactly once** 
  * ??





## Kafka Broker Discovery 

* MGMT 서버가 별도로 존재하지 않아도 된다(REAL???)
* 각 브로커는 **bootstrap server** 라고 불리는다. 
  * 이는 모든 브로커가 다른 모든 브로커의 정보를 가지고 있기 때문이다. 
  * 어떤 브로커가 어떤 토픽의 어떤 파티션을 가지는지 <= 라는 메타정보
* **So** 클라이언트는 오로지 하나의 브로커에만 연결되면 된다. 

* Behind the Scene 에서 클라이언트API는 "연결된 브로커"에게 메타데이터 정보를 질의하고, 이 정보로 
* 클라이언트는 다시 브로커에 접근함.


13![image](https://user-images.githubusercontent.com/34915108/117965459-c007d000-b35d-11eb-9a48-88c609276a43.png)






## Zookeeper



* 주키퍼는 브로커들을 관리한다. (브로커 리스트 등)
* 주키퍼 자체로 클러스터 형태(홀수개)로 운용된다. 
* 각 브로커는 주키퍼에 붙는다.(WHY?)
* 그러나 주키퍼는 fail-over 용도 등에 활용되고,
* 클라이언트로의 메타데이터 서빙은 브로커가 직접 담당한다. 


14![image](https://user-images.githubusercontent.com/34915108/117965480-c433ed80-b35d-11eb-9fde-74a1c59e6a06.png)




## 정리

![15](/Users/user/Desktop/카프카스터디/15.png)
